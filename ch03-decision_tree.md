##__1부 분류__


###__3장 의사결정 트리(Decision Tree)__

스무고개 게임은 '예' 혹은 '아니오'라는 답변을 가지고 어떤 것을 추측해내는 게임이다.  
이번 장에서 다루는 '의사결정 트리'는 마치 스무고개 게임처럼 동작하는 알고리즘이다.  
2010년 한 논문에 따르면, 의사결정 트리는 분류 기술 중 가장 많이 사용되는 기술이라고 한다.  

의사결정 트리는 '의사결정 영역', '단말 영역', '가지'로 구성된다.  
밑의 그림 1 예제의 사각형 영역이 의사결정 영역, 타원형 영역이 단말 영역, 의사결정 영역에서 뻗아나간 화살표가 가지이다.  

그림 1

![flowchart](https://s24.postimg.org/appmg2wad/image.png)  

의사결정 영역에서 질문을 하고, 해당 질문에 맞는 가지를 따라 이동하여 단말 노드에서 어떤 결론에 도달하게 되는 것이 의사결정 트리의 흐름이다.  

2장에서 다룬 k-최근접 이웃 알고리즘은 분류를 위해 데이터에 대한 통찰력이 요구되는 반면, 의사결정 트리는 데이터에 대한 통찰력이 요구되지 않는다.  
의사결정 트리의 장점 중 가장 좋은 것은 분류 결과를 사람이 쉽게 이해할 수 있다는 것이다.  

####3.1 트리 구조

|| 의사결정 트리 |
| :---: | :---: |
| 장점 | 계산 비용이 적다. |
| 단점 | '과적합(overfitting)'되기 쉽다. |
| 적용 | 수치형 값, 명목형 값 |

의사결정 트리는 '정보 이론(information theory)'을 이용하여 데이터 집합을 분할한다.  

의사결정 트리를 만들기 위해서는 최초의 의사결정이 하나 있어야 한다. 이 의사결정은 데이터를 분할하는 데 영향을 주는 '속성'으로 결정된다.  
이 속성은 모든 속성에 대해 반복적으로 계산하여 결정하는데, 결과가 가장 좋은 속성에 대해 분할을 시도한다.  
같은 방법으로 데이터 집합을 하위 집합으로 계속 분할하는데, 가지에 있는 데이터가 모두 같은 분류 항목에 속한다면 제대로 분류가 된 것이다.  

|| 의사결정 트리의 일반적인 접근 |
| :---: | :---: |
| 수집 | 모든 방법 |
| 준비 | 의사결정 트리는 명목형 값만 처리할 수 있기 때문에 연속형 값은 양자화 되어야 한다. |
| 분석 | 모든 방법. 트리를 만든 후 시각적으로 검토한다. |
| 훈련 | 트리 형태로 데이터를 구축한다. |
| 검사 | 학습된 트리를 가지고 오류율을 계산한다. |
| 사용 | 모든 지도학습에서 사용될 수 있다. 트리는 종종 데이터를 더 잘 이해하기 위해서 사용된다. |

일부 의사결정 트리는 데이터를 이진 데이터로 분할하지만, 이 장에서는 그렇게 하지 않는다.  
이진 데이터가 아닌, 4개의 데이터를 갖는 속성으로 분할하더라도 가지의 수만 늘어날 뿐 의사결정 트리의 큰 구조가 변경되진 않는다.  
이 장에서는 다양한 의사결정 트리 알고리즘 중 ID3 알고리즘을 사용한다.  
ID3 알고리즘을 요약하면 다음과 같다.(더 많은 정보는 (http://en.wikipedia.org/wiki/ID3_algorithm)에서 볼 수 있다.)

1. 데이터 집합 S 의 모든 속성의 '엔트로피(entropy)'를 계산한다.  
2. 엔트로피가 최소가 되는 속성(혹은 '정보 이득'이 가장 큰 속성)으로 집합을 분할한다.  
3. 해당 속성을 포함하는 노드로 결정 트리를 만든다.  
4. 나머지 속성을 사용하여 하위 집합에 대해 반복한다.  

####3.1.1 정보 이득

데이터 집합을 분할하는 방법에는 여러가지가 있다.  
그 중 우리는 한 가지 방법을 선정하여 체계적이지 못한 데이터를 체계적으로 만든다.  
다루기 어려운 데이터들을 조직화하기 위해 우리는 정보 이론을 이용한다.  
이를 이용하면 데이터를 분할하기 전과 후의 정보를 측정할 수 있다.  

데이터를 분할하기 전과 후의 변화를 정보 이득(information gain) 이라고 한다.  
정보 이득을 계산하면 어떤 속성이 데이터를 가장 잘 분할하는 지 확인할 수 있다.  
정보 이득이 가장 높은 속성을 가지고 분할하는 것이 가장 좋은 선택이다.  

데이터 집합에 대한 정보 측정 방법을 '섀넌 엔트로피(Shannon entropy)' 혹은 엔트로피라고 한다.  
엔트로피는 정보에 대한 기대 값으로 정의한다.  
엔트로피가 높다는 것은 데이터가 혼잡하다는 것을 뜻한다.  

![equation](https://latex.codecogs.com/gif.latex?l%28x_i%29%20%3D%20log_2%20p%28x%29)  
![](https://latex.codecogs.com/gif.latex?x_i)는 정보이고, ![](https://latex.codecogs.com/gif.latex?p%28x_i%29)를 ![](https://latex.codecogs.com/gif.latex?x_i)라는 분류 항목이 선택될 확률이라 한다.  

엔트로피를 계산하기 위해서는 분류 항목의 가능한 모든 값에 대해 모든 정보의 기대 값이 필요한데, 아래와 같은 식으로부터 얻을 수 있다.

![equation](https://latex.codecogs.com/gif.latex?H%20%3D%20-%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p%28x_i%29%20log_2%20p%28x_i%29)  

![](https://latex.codecogs.com/gif.latex?n)은 분류 항목의 개수이다.

---
