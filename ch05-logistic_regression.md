##__1부 분류__


###__로지스틱 회귀(Logistic Regression)__

이번 장에서는 처음으로 최적화 알고리즘을 다루게 된다.  
최적화 알고리즘은 우리 생활 속에 많은 관련이 있다.  
가장 빠른 시간 안에 A지점에서 B지점까지 가려면 어떻게 가야 할지에 대한 문제나, 가장 적게 일하고 가장 많은 돈은 벌려면 어떻게 해야 하는가에 대한 문제들이 최적화와 관련된 문제이다.  
이 장에서는 분류를 위해 비선형(nbonlinear) 함수를 훈련하는 최적화 알고리즘을 소개할 것이다.  

'회귀'는 여러 데이터들의 최적선을 그리는 것이다.  
로지스틱 회귀에서는 우리가 가지고 있는 데이터 집합을 분류하기 위해 방정식을 구한다.  
이 방정식에 대한 자세한 설명은 이 장에서 자세히 다루지는 않는다.  
이 장에서는 방정식의 알맞은 매개변수를 찾는 것에 중점을 두는데 이 '알맞은'이라는 이름이 회귀라는 이름의 근원이다.  
또한 최적화 알고리즘을 공부하며 기울기 상승(gradient ascent)을 다루게 되고, 확률적 기울기 상승을 조절하는 것에 대해서도 다루게 된다.  

####5.1 로지스틱 회귀와 시그모이드 함수로 분류하기

|| 로지스틱 회귀 |
| :---: | :---: |
| 장점 | 계산 비용이 적고, 구현하기 쉬우며, 결과 해석을 위한 지식 표현이 쉽다. |
| 단점 | 언더피팅(underfitting) 경향이 있어, 정확도가 낮게 나올 수 있다. |
| 활용 | 수치형 값, 명목형 값 |

우리는 필요한 속성을 모두 얻을 수 있고, 분류 항목을 예측할 수 있는 방정식을 필요로 한다.  
분류 항목이 두 개인 경우, 함수는 0 또는 1만을 결과로 출력한다.  
헤비사이드(Heaviside) 함수(계단 함수)의 경우 0과 1 사이의 값을 너무 급격하게 움직이는데 이로 인해 때때로 분할이 어려울 때가 있다.  
이런 함수와 비슷하게 동작하지만 분할하기 쉬운 시그모이드(sigmoid)라는 함수가 있다.  
![equation](https://latex.codecogs.com/gif.latex?s%28x%29%20%3D%20%5Cfrac%7B1%7D%7B1%20&plus;%20e%5E-x%20%7D)  
시그모이드 함수는 값이 ![equation](https://latex.codecogs.com/gif.latex?x)의 값이 증가하면 1에 가까워지고, 값이 감소하면 0에 가까워진다.  
로지스틱 회귀 분류기를 위해 가지고 있는 각각의 속성에 가중치를 곱한 다음 서로 더하고, 이 수를 시그모이드 함수에 넣어 분류하게 된다.  
해당 값이 0.5 이상이면 1로 분류되고, 0.5 미만이라면 0으로 분류된다.  
우리는 가장 좋은 가중치를 구해야 한다.  

####5.2 가장 좋은 회귀 계수를 찾기 위해 최적화 사용하기

![equation](https://latex.codecogs.com/gif.latex?z%20%3D%20w_0x_0%20&plus;%20w_0x_1%20&plus;%20w_2x_2%20&plus;%20...%20&plus;%20w_nx_n)

위 식은 ![equation](https://latex.codecogs.com/gif.latex?z%20%3D%20w%5ET%20x)와 같다.  
우리는 가장 좋은 계수 ![equation](https://latex.codecogs.com/gif.latex?w)를 찾고자 한다.  
이를 위해 우리는 기울기 상승을 통한 최적화를 알아볼 것이다.  

####5.2.1 기울기 상승

기울기 상승의 아이디어는 함수에서 최대 지점을 찾고자 할 때, 이 지점으로 이동하는 가장 좋은 방법이 기울기의 방향에 있다는 것을 기본으로 한다.  
함수 ![equation](https://latex.codecogs.com/gif.latex?f%28x%2Cy%29)의 기울기는 다음과 같은 방정식으로 표현된다.  
![equation](https://s27.postimg.org/b6e8mdaer/image.png)

해당 방정식은 분자 만큼 ![equation](https://latex.codecogs.com/gif.latex?x) 방향으로, 분모 만큼 ![equation](https://latex.codecogs.com/gif.latex?y) 방향으로 이동하는 것을 의미한다.

함수 ![equation](https://latex.codecogs.com/gif.latex?f%28x%2Cy%29)는 따로 정의가 필요하고, 기울기를 구하는 지점에서 미분이 가능해야 한다.  
지금까지 기울기가 이동할 방향에 대해 다루었고, 크기에 대해서는 다루지 않았다.  
크기는 매개변수 ![equation](https://latex.codecogs.com/gif.latex?a)에 의해 제공되고 다음과 같이 벡터로 표현할 수 있다.  
![equation](https://latex.codecogs.com/gif.latex?w%3A%3D%20w&plus;a%20%5Cbigtriangledown%20_w%20f%28w%29)  
이런 단계들은 멈춤 조건(단계의 반복 횟수를 만족하거나 지정한 오차의 범위 이내)에 도달할 때 까지 되풀이된다.  

그림 5.1

![GradientAscent](https://s29.postimg.org/rb2wh5bh3/image.png)

그림 5.1과 같이 함수의 최대 지점을 찾아간다.  

####5.2.1 기울기 상승을 사용하여 가장 좋은 매개변수 찾기

__의사코드__

모든 가중치를 1로 설정한다.  
R번 반복한다.  
&nbsp;&nbsp;입력 데이터 집합의 기울기를 계산한다.  
&nbsp;&nbsp;![equation](https://latex.codecogs.com/gif.latex?a%20*%20w)로 가중치 벡터(![equation](https://latex.codecogs.com/gif.latex?w))를 변경한다.  
가중치 벡터를 반환한다.  

####5.2.2 확률적인 기울기 상승

__의사코드__
모든 가중치를 1로 설정한다.  
데이터 집합 내에 각각 데이터에 대해 반복한다.  
&nbsp;&nbsp;데이터 중 하나의 기울기를 계산한다.
&nbsp;&nbsp;![equation](https://latex.codecogs.com/gif.latex?a%20*%20w)로 가중치 벡터(![equation](https://latex.codecogs.com/gif.latex?w))를 변경한다.  
가중치 벡터를 반환한다.  

앞선 5.2.1의 의사코드는 수많은 데이터의 수많은 속성을 가진 경우에는 너무나 많은 계산을 하게 된다.  
이 점을 개선하기 위해 '확률 기울기 상승'에서는 한번에 한 사례에 대해서만 가중치를 갱신한다.  
즉 확률 기울기 상승은 '온라인 학습 알고리즘'이다.  
온라인 학습 알고리즘의 경우는 한번에 전체 데이터 집합에 대해 학습하지 않고, 하나의 사례에 대해서만 학습하는 알고리즘이다.  
반면 5.2.1의 알고리즘은 '오프라인 학습 알고리즘'으로 한번에 전체 데이터 집합에 대해 학습하게 된다.  
전체 데이터를 한번에 처리하는 방법을 일괄 처리(batch processing)라 하고,
일괄 처리는 새로운 데이터가 추가되었을 때 갱신하기 위해 너무 많은 비용을 소모하는 경향이 있다.  
이를 확률 기울기 상승에서는 새로운 데이터가 추가되었을 때 점진적으로 분류기를 갱신하도록 하여 개선하였다.  

이러한 최적화 알고리즘들의 평가 방법은 이것이 얼마나 잘 '수렴'하는지에 대해 확인하는 것이다.  
이러한 관점에서 가중치 벡터 변환 시의 ![equation](https://latex.codecogs.com/gif.latex?a)값이 고정되어 있다면 그림 5.2와 같은 상황이 일어난다.  

![plot](https://s29.postimg.org/lzlbz0kon/image.png)

그림 5.2와 같은 플롯에서 확인할 수 있는 점은 안정값을 찾은 후부터 큰 변동은 없어졌지만 주기적인 작은 변동이 남아있다는 것이다.  
즉, 아직 확실히 분류되지 않은 데이터들이 있다는 것이다.  
또한 이 데이터들이 가중치에 큰 변화를 준다는 것이다.  
이러한 문제를 해결하기 위해 ![equation](https://latex.codecogs.com/gif.latex?a) 값을 반복 횟수에 따라 감소시킨다.  
하지만 ![equation](https://latex.codecogs.com/gif.latex?a)값이 0이 되는 것을 방지하기 위해 일정 수준의 상수를 더해주어야 한다.  
이러한 방법을 통해 수렴시키면 그림 5.3과 같이 매우 빠르게 수렴하게 된다.  

그림 5.3

![plot](https://s30.postimg.org/ycw5ejvkx/image.png)

####5.3.1 데이터에서 누락된 값 다루기

많은 속성을 가진 데이터를 사용하다 보면 속성들 중 일부가 누락된 경우가 있다.  
이런 경우를 해결하기 위해서 보통 아래와 같은 선택을 한다.  

1. 사용 가능한 모든 데이터에서 속성의 평균값을 사용한다.  
2. 알려지지 않은 값은 -1과 같은 특별한 값으로 채운다.  
3. 사례를 무시한다.  
4. 유사한 아이템들로부터의 평균값을 사용한다.  
5. 값을 예측하기 위해 다른 기계 학습 알고리즘을 사용한다.  

####5.3.2 로지스틱 회귀로 분류하기

앞서 설명해온 최적화 알고리즘을 통해 분류를 할 수 있다.  
로지스틱 회귀를 가지고 사례를 분류하는 것은 간단하다.  
앞서 최적화된 가중치로 곱해진 검사 벡터를 시그모이드 함수로 계산하고, 함수의 값이 0.5보다 크다면 1을, 작다면 0으로 분류할 수 있다.  

로지스틱 회귀는 시그모이드라는 비선형 함수의 최적의 매개변수를 찾는다.  
최적의 매개변수를 찾기 위해 기울기 상승 알고리즘을 이용하였고, 이를 개선한 확률 기울기 상승으로 최적의 매개변수를 찾을 수 있다.  

---
